## Training

### 随机梯度下降(SGD)
 输入一批样本， 求出这些样本的梯度平均值， 根据平均值改变参数, 称为Batch或者Mini-Batch, batch的样本数量大致设置为50~200不等.

### 训练数据初始化
 常用的归一化:
$$
newX = \frac{X - mean(X)}{std(X)}
$$

### 参数初始化
初始化的原则是：$W^TX +b$一开始就在零附近.  
一种比较简单的方法是:  $(W, b)$初始化从区间$(-\frac{1}{\sqrt {d}}, \frac{1}{\sqrt {d}})$均匀随机取值， 其中$d$为所在层的神经元个数, 如果$X$服从正态分布， 均值0， 方差1， 且各个维度无关， 则
$W^TX +b$是均值为0， 方差为$\frac{1}{3}$的正态分布.

### Batch Normalization
TODO

### 目标函数
* SoftMax
* 交叉熵

## Reference
* 浙江大学机器学习课程 24 - 胡浩基
